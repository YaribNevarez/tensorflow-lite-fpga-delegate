{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbiM6lilQxtQ",
        "outputId": "ede20a3d-6cc7-4d96-c754-cbc1a79a6008"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-08 08:35:20.569652: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
            "2021-11-08 08:35:20.569681: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import SeparableConv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import os\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Jupyter notebook to build, train, and deply CNN models for TF Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IuN7OgXSQ9i2"
      },
      "outputs": [],
      "source": [
        "filename = \"tiny_vgg_q\" # Set the name for the model output\n",
        "\n",
        "training_batch_size = 20\n",
        "training_epochs = 20\n",
        "\n",
        "early_stop_training = True\n",
        "quantize_aware_training = True\n",
        "\n",
        "target_exponent = 5 # Target exponent bit size for custom floating-point \n",
        "target_mantissa = 2 # Target mantissa bit size for custom floating-point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "google_colab = False # Set True when using on google colab\n",
        "if google_colab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN ARCHITECTURE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define cnn model\n",
        "def define_model_CNN():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(40, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(50, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(60, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(120, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(120, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\t#opt = SGD(learning_rate=0.001, momentum=0.9)\n",
        "\topt = \"adam\"\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantized aware training method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fh2Y7AaGvoUK"
      },
      "outputs": [],
      "source": [
        "import struct\n",
        "\n",
        "def bin2float(b):\n",
        "    ''' Convert binary string to a float.\n",
        "\n",
        "    Attributes:\n",
        "        :b: Binary string to transform.\n",
        "    '''\n",
        "    h = int(b, 2).to_bytes(8, byteorder=\"big\")\n",
        "    return struct.unpack('>d', h)[0]\n",
        "\n",
        "\n",
        "def float2bin(f):\n",
        "    ''' Convert float to 64-bit binary string.\n",
        "\n",
        "    Attributes:\n",
        "        :f: Float number to transform.\n",
        "    '''\n",
        "    [d] = struct.unpack(\">Q\", struct.pack(\">d\", f))\n",
        "    return f'{d:064b}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7Y8p19yiv2rs"
      },
      "outputs": [],
      "source": [
        "def quantize_float (float_number, exponent_bits, mantissa_bits):\n",
        "    exponent_sign = 1\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
        "    if mantissa_bits < 0:\n",
        "      mantissa_bits = 0\n",
        "    if exponent_bits < 0:\n",
        "      exponent_bits = 0\n",
        "      \n",
        "    coefficient = float2bin(float_number)\n",
        "    sign = int (coefficient[:1], 2)\n",
        "    if 0 < mantissa_bits:\n",
        "        custom_mantissa = int (coefficient[12 : 12 + mantissa_bits], 2)\n",
        "    else:\n",
        "        custom_mantissa = 0\n",
        "    residual_mantissa = int (coefficient[12 + mantissa_bits:], 2)\n",
        "    exponent = int (coefficient[1:12], 2) - 1023\n",
        "\n",
        "    exponent_full_range = pow(2, exponent_bits - exponent_sign) - 1\n",
        "    if exponent < - exponent_full_range:\n",
        "        quantized_value = 0\n",
        "    elif exponent > exponent_full_range:\n",
        "        quantized_value = pow(-1, sign) * (1 + (1 - pow(2, - mantissa_bits))) * pow(2, exponent_full_range)\n",
        "    else:\n",
        "        if (pow (2, (52 - (mantissa_bits + 1))) - 1) < residual_mantissa:\n",
        "            custom_mantissa += 1\n",
        "            if (pow (2, mantissa_bits) - 1) < custom_mantissa:\n",
        "                custom_mantissa = 0\n",
        "                exponent += 1\n",
        "    \n",
        "        quantized_value = pow(-1, sign) * (1 + custom_mantissa * pow(2, - mantissa_bits)) * pow(2, exponent)\n",
        "    return quantized_value\n",
        "\n",
        "def quantize_model(model, exponent_bits, mantissa_bits):\n",
        "  for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "      layer_weight = layer.get_weights()\n",
        "      if len(layer_weight) == 2:\n",
        "        filter_matrix = layer_weight[0]\n",
        "        bias_matrix = layer_weight[1]\n",
        "        if filter_matrix.ndim == 4:\n",
        "          for id_i, i in enumerate(filter_matrix):\n",
        "            for id_j, j in enumerate(i):\n",
        "              for id_k, k in enumerate(j):\n",
        "                for id_l ,l in enumerate(k):\n",
        "                  filter_matrix[id_i][id_j][id_k][id_l] = quantize_float (l, exponent_bits, mantissa_bits)\n",
        "                  bias_matrix[id_l] = quantize_float (bias_matrix[id_l], exponent_bits, mantissa_bits)\n",
        "        layer_weight[0] = filter_matrix\n",
        "        layer_weight[1] = bias_matrix\n",
        "        layer.set_weights(layer_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Early stop callback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vaA7sG9xenUF"
      },
      "outputs": [],
      "source": [
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto', restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mHKtRDtPn0eQ"
      },
      "outputs": [],
      "source": [
        "mantissa_bits = target_mantissa\n",
        "exponent_bits = target_exponent\n",
        "epoch_count = 0\n",
        "quantize = False\n",
        "class MyCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    global epoch_count\n",
        "    global quantize\n",
        "    global mantissa_bits\n",
        "    epoch_count += 1\n",
        "    quantize = 1 < epoch_count\n",
        "    if quantize:\n",
        "      quantize_model(self.model, exponent_bits, mantissa_bits)\n",
        "      #mantissa_bits -= 1\n",
        "  def on_train_end(self, logs={}):\n",
        "    quantize_model(self.model, exponent_bits, mantissa_bits)\n",
        "  def on_batch_end(self, epoch, logs=None):\n",
        "    if quantize:\n",
        "      quantize_model(self.model, exponent_bits, mantissa_bits)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WPVVK7cLRHlR"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(filename):\n",
        "  os.mkdir(filename)\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "\ttrainY = to_categorical(trainY)\n",
        "\ttestY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "\n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tpyplot.savefig(filename + '/' + filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        "\n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "\t# load dataset\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# prepare pixel data\n",
        "\ttrainX, testX = prep_pixels(trainX, testX)\n",
        "\t# define model\n",
        "\tmodel = define_model_CNN()\n",
        "\t# fit model\n",
        "\thistory = model.fit(trainX, trainY, epochs=20, batch_size=64, validation_data=(testX, testY), verbose=1, callbacks=[MyCallback()])\n",
        "\t# evaluate model\n",
        "\t_, acc = model.evaluate(testX, testY, verbose=0)\n",
        "\tprint('> %.3f' % (acc * 100.0))\n",
        "\t# learning curves\n",
        "\tsummarize_diagnostics(history)\n",
        "\t# Save the entire model to a HDF5 file.\n",
        "\tmodel.save(filename + \"/\" + filename + '.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-08 08:35:26.846550: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2021-11-08 08:35:26.857480: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-08 08:35:26.858152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0\n",
            "coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\n",
            "2021-11-08 08:35:26.858263: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
            "2021-11-08 08:35:26.858335: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 08:35:26.859254: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2021-11-08 08:35:26.859482: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2021-11-08 08:35:26.859549: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 08:35:26.859609: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 08:35:26.859673: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
            "2021-11-08 08:35:26.859682: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2021-11-08 08:35:26.860068: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-08 08:35:26.885632: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2494430000 Hz\n",
            "2021-11-08 08:35:26.886370: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55af26ca2800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-11-08 08:35:26.886389: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-11-08 08:35:26.888799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-11-08 08:35:26.888814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      \n"
          ]
        }
      ],
      "source": [
        "# entry point, run the test harness\n",
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "\n",
        "# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)\n",
        "\n",
        "# define model\n",
        "model = define_model_CNN()\n",
        "# fit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsUR8C6RRJVc",
        "outputId": "f4b2312f-ee7b-41d7-8715-6afdbcdb31b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-08 08:35:27.280719: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 614400000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2500/2500 [==============================] - 82s 33ms/step - loss: 1.3385 - accuracy: 0.5161 - val_loss: 1.0987 - val_accuracy: 0.6077\n",
            "Epoch 2/20\n",
            "2500/2500 [==============================] - 76s 30ms/step - loss: 0.9409 - accuracy: 0.6676 - val_loss: 0.9174 - val_accuracy: 0.6757\n",
            "Epoch 3/20\n",
            "2500/2500 [==============================] - 2313s 925ms/step - loss: 0.7027 - accuracy: 0.7525 - val_loss: 0.8066 - val_accuracy: 0.7203\n",
            "Epoch 4/20\n",
            "2500/2500 [==============================] - 2303s 921ms/step - loss: 0.6269 - accuracy: 0.7784 - val_loss: 0.8090 - val_accuracy: 0.7204\n",
            "Epoch 5/20\n",
            " 536/2500 [=====>........................] - ETA: 33:47 - loss: 0.5397 - accuracy: 0.8121"
          ]
        }
      ],
      "source": [
        "\n",
        "if not early_stop_training and not quantize_aware_training:\n",
        "    history = model.fit(trainX, trainY, epochs=training_epochs, batch_size=training_batch_size, validation_data=(testX, testY), verbose=1)\n",
        "elif not early_stop_training and quantize_aware_training:\n",
        "    history = model.fit(trainX, trainY, epochs=training_epochs, batch_size=training_batch_size, validation_data=(testX, testY), verbose=1, callbacks=[MyCallback()])\n",
        "elif early_stop_training and not quantize_aware_training:\n",
        "    history = model.fit(trainX, trainY, epochs=training_epochs, batch_size=training_batch_size, validation_data=(testX, testY), verbose=1, callbacks=[monitor])    \n",
        "elif early_stop_training and quantize_aware_training:\n",
        "    history = model.fit(trainX, trainY, epochs=training_epochs, batch_size=training_batch_size, validation_data=(testX, testY), verbose=1, callbacks=[MyCallback(), monitor])\n",
        "else:\n",
        "    print (\"Impossible Error!\")\n",
        "\n",
        "# evaluate model\n",
        "_, acc = model.evaluate(testX, testY, verbose=0)\n",
        "print('> %.3f' % (acc * 100.0))\n",
        "# learning curves\n",
        "summarize_diagnostics(history)\n",
        "# Save the entire model to a HDF5 file.\n",
        "model.save(filename + \"/\" + filename + '.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCLycpCTcGdR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Predicted: [[0.08878151 0.08323798 0.10533294 0.11452007 0.13186267 0.09694849\n",
            "  0.11534996 0.08052979 0.09734241 0.08609421]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "img_path = \"dog.jpg\"\n",
        "img = image.load_img(img_path, target_size=(32, 32))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = x.astype('float32')\n",
        "x = x / 255.0\n",
        "\n",
        "model = load_model(filename + \"/\" + filename + '.h5')\n",
        "\n",
        "output_data = model.predict(x)\n",
        "\n",
        "print('TensorFlow Predicted:', output_data)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_model(model):\n",
        "  for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "      layer_weight = layer.get_weights()\n",
        "      if len(layer_weight) == 2:\n",
        "        filter_matrix = layer_weight[0]\n",
        "        bias_matrix = layer_weight[1]\n",
        "        print (filter_matrix)\n",
        "        print (bias_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.546875]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]\n",
            "\n",
            "  [[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]\n",
            "\n",
            "  [[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]]\n",
            "\n",
            "\n",
            " [[[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]\n",
            "\n",
            "  [[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]\n",
            "\n",
            "  [[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]]\n",
            "\n",
            "\n",
            " [[[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]\n",
            "\n",
            "  [[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]\n",
            "\n",
            "  [[0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]\n",
            "   [0.       0.       0.       ... 0.       0.       0.      ]]]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0.]\n",
            "[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[[[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]\n",
            "\n",
            "\n",
            " [[[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "  [[0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   ...\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]\n",
            "   [0. 0. 0. ... 0. 0. 0.]]]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "> 10.000\n"
          ]
        }
      ],
      "source": [
        "print_model (model)\n",
        "_, acc = model.evaluate(testX, testY, verbose=0)\n",
        "print('> %.3f' % (acc * 100.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmFfu4r-dFdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 40)        1120      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 40)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 16, 16, 50)        18050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 50)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 8, 8, 60)          27060     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 60)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 120)         64920     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1920)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 120)               230520    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1210      \n",
            "=================================================================\n",
            "Total params: 342,880\n",
            "Trainable params: 342,880\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-08 02:34:19.768577: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1228800000 exceeds 10% of free system memory.\n",
            "2021-11-08 02:34:31.226327: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 614400000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/yarib/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /home/yarib/anaconda3/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-08 02:35:56.515847: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpm5bspbx7/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-08 02:35:57.443127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-08 02:35:57.449032: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2021-11-08 02:35:57.454210: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
            "2021-11-08 02:35:57.581683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-08 02:35:57.584470: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b3adde33f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-11-08 02:35:57.584487: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\n",
            "2021-11-08 02:35:57.585986: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-08 02:35:57.586740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0\n",
            "coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\n",
            "2021-11-08 02:35:57.615001: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.616627: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.616660: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2021-11-08 02:35:57.616686: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2021-11-08 02:35:57.617602: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.617694: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.620251: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.620264: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2021-11-08 02:35:57.620289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-11-08 02:35:57.620297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2021-11-08 02:35:57.620308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2021-11-08 02:35:57.645326: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\n",
            "2021-11-08 02:35:57.645350: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.82ms.\n",
            "2021-11-08 02:35:57.645356: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-11-08 02:35:57.782867: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\n",
            "2021-11-08 02:35:57.782897: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\n",
            "2021-11-08 02:35:57.826420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-08 02:35:57.827213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0\n",
            "coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\n",
            "2021-11-08 02:35:57.827344: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.827422: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.827448: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2021-11-08 02:35:57.827465: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2021-11-08 02:35:57.827526: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.827576: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.827628: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:57.827636: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2021-11-08 02:35:57.827648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-11-08 02:35:57.827652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2021-11-08 02:35:57.827657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, utils\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "  print(\"Folder \" + filename + \" does not exist.\")\n",
        "  exit()\n",
        "\n",
        "# Recreate the exact same model, including its weights and the optimizer\n",
        "model = tf.keras.models.load_model(filename + \"/\" + filename + '.h5')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "tf.keras.utils.plot_model(model, filename + \"/\" + filename + \".png\", show_shapes=True, show_layer_names=True, expand_nested=True)\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255 , test_images / 255\n",
        "\n",
        "train_images = tf.cast(train_images, tf.float32)\n",
        "cifar_ds = tf.data.Dataset.from_tensor_slices((train_images)).batch(1)\n",
        "def representative_dataset():\n",
        "  for input_value in cifar_ds.take(100):\n",
        "    # Model has only one input so each data point has one element.\n",
        "    yield [input_value]\n",
        "\n",
        "# Convert the model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "#converter.representative_dataset = representative_dataset\n",
        "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.float32  # or tf.uint8\n",
        "converter.inference_output_type = tf.float32  # or tf.uint8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open(filename + \"/\" + filename + \"_f32\" + '.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INb151fxeI6B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmprryhndhn/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmprryhndhn/assets\n",
            "2021-11-08 02:35:59.145633: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-08 02:35:59.146295: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "2021-11-08 02:35:59.146409: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
            "2021-11-08 02:35:59.147160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-11-08 02:35:59.147779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0\n",
            "coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 74.65GiB/s\n",
            "2021-11-08 02:35:59.147891: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:59.147963: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:59.147990: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2021-11-08 02:35:59.148008: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2021-11-08 02:35:59.148061: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:59.148117: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:59.148174: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
            "2021-11-08 02:35:59.148181: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2021-11-08 02:35:59.148193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-11-08 02:35:59.148198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2021-11-08 02:35:59.148202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2021-11-08 02:35:59.150907: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize\n",
            "2021-11-08 02:35:59.150920: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.\n",
            "2021-11-08 02:35:59.150924: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.\n",
            "2021-11-08 02:35:59.204000: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.\n",
            "2021-11-08 02:35:59.204041: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.\n",
            "2021-11-08 02:35:59.380388: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 614400000 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Convert the model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.float32  # or tf.uint8\n",
        "converter.inference_output_type = tf.float32  # or tf.uint8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open(filename + \"/\" + filename + \"_i8\" + '.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ4KikJ-fkaU"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "tensorflow/lite/kernels/kernel_util.cc:154 scale_diff / output_scale <= 0.02 was not true.Node number 9 (FULLY_CONNECTED) failed to prepare.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_23556/1540727378.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load TFLite model and allocate tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_i8\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get input and output tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py\u001b[0m in \u001b[0;36mallocate_tensors\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAllocateTensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_safe_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: tensorflow/lite/kernels/kernel_util.cc:154 scale_diff / output_scale <= 0.02 was not true.Node number 9 (FULLY_CONNECTED) failed to prepare.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=filename + \"/\" + filename + \"_i8\" + '.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpXHwv2Tf3yI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Lite Predicted: [[0.         0.         0.1015625  0.01171875 0.44921875 0.39453125\n",
            "  0.01171875 0.02734375 0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], x)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print('TensorFlow Lite Predicted:', output_data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "quantized_training.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3ba63467901cd6d3991f497c38810e6d1156dd2dfb6eb0edc80f01dd9606bacd"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
